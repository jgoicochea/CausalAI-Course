{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using CSV\n",
    "using Distributions\n",
    "using DataFrames\n",
    "using Dates\n",
    "using Plots\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using LaTeXStrings\n",
    "using Lasso\n",
    "# using MLBase\n",
    "using Statistics\n",
    "using GLMNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory\n",
    "cd(\"D:/PUCP/JP-TC\")\n",
    "\n",
    "# Load the necessary packages\n",
    "using CSV, DataFrames\n",
    "\n",
    "# Read the data\n",
    "data = CSV.read(\"wage2015_subsample_inference.csv\", DataFrame);\n",
    "\n",
    "# Separate the features and the target variable\n",
    "data = select(data, Not([\"wage\", \"rownames\"])); # Drop columns 'wage' and 'lwage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the design matrix\n",
    "\n",
    "design = @formula( lwage ~ 0 + sex + (exp1 + exp2 + exp3 + exp4 + hsg + scl + clg + ad + \n",
    "so + we + ne + occ2 + ind2))\n",
    "X_flexible = modelmatrix(design, data);                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5150×1 Matrix{Float64}:\n",
       " 2.2633643798407643\n",
       " 3.872802292274865\n",
       " 2.403126322215923\n",
       " 2.634927936273247\n",
       " 3.361976668508874\n",
       " 2.4622152385859297\n",
       " 2.9565115604007097\n",
       " 2.9565115604007097\n",
       " 2.4849066497880004\n",
       " 2.9565115604007097\n",
       " ⋮\n",
       " 3.117779707996832\n",
       " 2.822980167776187\n",
       " 3.1796551117149194\n",
       " 2.6280074934286737\n",
       " 2.6925460145662448\n",
       " 3.138833117194664\n",
       " 3.649658740960655\n",
       " 3.4955080611333966\n",
       " 2.8511510447428834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [data[:,1];;]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Creating the Lasso Cross-Validation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The `log_grid` function is pretty straight forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_grid (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_grid(lower::Int, upper::Int, log_step::Float64)\n",
    "    log_grid = range(lower, stop=upper, length= Int(1 /log_step))\n",
    "    return exp.(log_grid)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To code the `k_folds` function, there are many different approaches. However, we sticked to using only numpy. With this library, we exploited the kronecker product operation and block matrices to build the $k$-folds. Also, we addressed the issue of divisibility between the sample size $n$ and $k$ using an if-else statement depending on the module of $n/k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_folds (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function k_folds(data::AbstractArray, k::Int = 5)\n",
    "    mdl = size(data, 1) % k\n",
    "    floor = size(data, 1) ÷ k \n",
    "\n",
    "    if mdl == 0\n",
    "        trues = fill(1, floor, 1)\n",
    "        split_matrix = kron(I(k), trues)\n",
    "    else\n",
    "        trues_g1 = fill(1, floor + 1, 1)\n",
    "        split_matrix_g1 = kron(I(mdl), trues_g1)\n",
    "        \n",
    "        trues_g2 = fill(1, floor, 1)\n",
    "        split_matrix_g2 = kron(I(k - mdl), trues_g2)\n",
    "        \n",
    "        split_matrix = [split_matrix_g1  zeros(size(split_matrix_g1, 1), size(split_matrix_g2, 2));\n",
    "                        zeros(size(split_matrix_g2, 1), size(split_matrix_g1, 2))  split_matrix_g2]\n",
    "    end\n",
    "    \n",
    "    sm_bool = split_matrix .== 1\n",
    "    splits = [sm_bool[:, x] for x in 1:k]\n",
    "    \n",
    "    return splits\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For the `optimal_lambda` search function, we basically adapted the code provided in the labs so it can use the functions of log-grid and our own $k$-folds function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimal_lambda (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using GLMNet\n",
    "function optimal_lambda(Y::AbstractVector, X::AbstractArray, lambda_bounds::Tuple{Int, Int}, k::Int = 5; niter::Int = 100)\n",
    "    Y = vec(Y) \n",
    "\n",
    "    if ndims(X) == 1\n",
    "        X = reshape(X, :, 1)\n",
    "    end\n",
    "\n",
    "    folds = k_folds(X, k)\n",
    "    all_lambdas = log_grid(lambda_bounds[1],lambda_bounds[2], 1/niter)\n",
    "    all_mse = zeros(niter)\n",
    "\n",
    "    for (j, l) in enumerate(all_lambdas)\n",
    "        split_pes = zeros(k)\n",
    "        \n",
    "        for i in 1:k\n",
    "            X_train = X[.!folds[i], :]\n",
    "            X_test = X[folds[i], :]\n",
    "            y_train = Y[.!folds[i]]\n",
    "            y_test = Y[folds[i]]\n",
    "\n",
    "            model = glmnet(X_train, y_train, alpha=1.0, lambda=[l])\n",
    "            predict = GLMNet.predict(model, X_test)\n",
    "\n",
    "            pe = sum((y_test - predict).^2)\n",
    "            split_pes[i] = pe\n",
    "        end\n",
    "\n",
    "        all_mse[j] = mean(split_pes)\n",
    "    end\n",
    "\n",
    "    selected = argmin(all_mse)\n",
    "    optimal_lambda = all_lambdas[selected]\n",
    "    optimal_model = glmnet(X, Y, alpha=1.0, lambda=[optimal_lambda])\n",
    "    optimal_coef = [optimal_model.a0;optimal_model.betas[:]]\n",
    "\n",
    "    output = Dict(\n",
    "        \"optimal_lambda\" => optimal_lambda,\n",
    "        \"optimal_coef\" => optimal_coef,  #issue\n",
    "        \"all_lambdas\" => all_lambdas,\n",
    "        \"all_mse\" => all_mse\n",
    "    )\n",
    "\n",
    "    return output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The `predict_model` function can be easily implemented using the results of `optimal_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function predict_model(optimal_model::Dict, X::AbstractArray)\n",
    "    intercept = ones(size(X, 1), 1)\n",
    "    Z = [intercept;; X]\n",
    "    \n",
    "    return Z * optimal_model[\"optimal_coef\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Applying the Lasso Cross-Validation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the sample in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = rand(Float64, size(data)[1]) .< 0.80\n",
    "test_sample = .!(train_sample)\n",
    "y_train, y_test = y[train_sample], y[test_sample]\n",
    "X_flexible_train, X_flexible_test = X_flexible[train_sample, :], X_flexible[test_sample, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We perform the OLS fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}:\n",
       "\n",
       "Coefficients:\n",
       "───────────────────────────────────────────────────────────────────────\n",
       "           Coef.  Std. Error       t  Pr(>|t|)    Lower 95%   Upper 95%\n",
       "───────────────────────────────────────────────────────────────────────\n",
       "x1    2.82293     0.0691773    40.81    <1e-99   2.68731      2.95856\n",
       "x2   -0.0951806   0.0163281    -5.83    <1e-08  -0.127193    -0.0631686\n",
       "x3    0.0281351   0.0115791     2.43    0.0151   0.00543366   0.0508365\n",
       "x4   -0.0838956   0.117094     -0.72    0.4737  -0.313463     0.145672\n",
       "x5    0.0156588   0.0437893     0.36    0.7207  -0.070192     0.10151\n",
       "x6   -0.00182871  0.00537928   -0.34    0.7339  -0.012375     0.0087176\n",
       "x7    0.117898    0.0519481     2.27    0.0233   0.0160513    0.219744\n",
       "x8    0.253109    0.0521105     4.86    <1e-05   0.150944     0.355274\n",
       "x9    0.543313    0.0529705    10.26    <1e-23   0.439462     0.647164\n",
       "x10   0.740253    0.0560882    13.20    <1e-38   0.630289     0.850216\n",
       "x11  -0.00954409  0.020856     -0.46    0.6473  -0.0504331    0.031345\n",
       "x12   0.0306639   0.0224585     1.37    0.1722  -0.0133669    0.0746947\n",
       "x13   0.00127674  0.0225202     0.06    0.9548  -0.0428752    0.0454287\n",
       "x14  -0.01852     0.00128894  -14.37    <1e-44  -0.021047    -0.015993\n",
       "x15  -0.0143222   0.00146991   -9.74    <1e-21  -0.0172041   -0.0114404\n",
       "───────────────────────────────────────────────────────────────────────\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using GLM, DataFrames\n",
    "\n",
    "# Fitting a linear regression model\n",
    "model_ls = lm([ones(size(X_flexible_train)[1]);;X_flexible_train],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Now we search the optimal lambda using our `optimal_lambda` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 4 entries:\n",
       "  \"optimal_coef\"   => [3.13263, -0.0549881, 0.00627277, 0.0, 0.0, 0.0, -0.09324…\n",
       "  \"all_mse\"        => [213.914, 213.864, 213.821, 213.753, 213.656, 213.549, 21…\n",
       "  \"optimal_lambda\" => 0.0177689\n",
       "  \"all_lambdas\"    => [0.000911882, 0.0010504, 0.00120996, 0.00139375, 0.001605…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finding the optimal lambda and fitting the Lasso model using the training data\n",
    "model_lasso = optimal_lambda(y_train, X_flexible_train, (-7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017768944609069942"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printing the optimal lambda\n",
    "model_lasso[\"optimal_lambda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Float64}:\n",
       "  3.132631959797634\n",
       " -0.05498805908555213\n",
       "  0.00627276757380166\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       " -0.09324321682448818\n",
       "  0.0\n",
       "  0.23665716689555433\n",
       "  0.4042965544988727\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       " -0.016992863063534744\n",
       " -0.010210126354513696"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printing the optimal coefficients\n",
    "model_lasso[\"optimal_coef\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now we use HDM for python (hdmpy) to estimate the model using the theoretically optimal penalty parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HDMjl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 15 entries:\n",
       "  \"tss\"          => 1348.46\n",
       "  \"dev\"          => [0.904667, -0.565009, 0.393842, -0.50592, -0.0116236, -0.01…\n",
       "  \"model\"        => [0.0 31.0 … 10.0 9.0; 0.0 18.0 … 19.0 4.0; … ; 1.0 12.0 … 1…\n",
       "  \"loadings\"     => [0.246062, 5.40921, 2.04982, 7.43547, 27.4597, 0.206141, 0.…\n",
       "  \"sigma\"        => 0.500159\n",
       "  \"lambda0\"      => 470.239\n",
       "  \"lambda\"       => [115.708, 2543.62, 963.907, 3496.45, 12912.6, 96.9354, 103.…\n",
       "  \"intercept\"    => 3.17201\n",
       "  \"iter\"         => 4\n",
       "  \"residuals\"    => [0.459347, -0.392823, 0.230223, -0.618978, 0.149122, 0.0272…\n",
       "  \"rss\"          => 1028.4\n",
       "  \"index\"        => Bool[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1]\n",
       "  \"beta\"         => [-0.0943658, 0.00874918, 0.0, 0.0, 0.0, -0.112374, 0.0, 0.2…\n",
       "  \"options\"      => Dict{String, Any}(\"intercept\"=>true, \"post\"=>true, \"meanx\"=…\n",
       "  \"coefficients\" => [3.17201, -0.0943658, 0.00874918, 0.0, 0.0, 0.0, -0.112374,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_rlasso = rlasso(X_flexible_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may notice, the optimal penalty parameter resulting from this procedure is not comparable in size to the cross validation result. This is due to the fact that this penalty is the theoretically optimal parameter for the Lasso estimator under data-driven penalty loadings. That is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\hat{\\beta} = \\arg \\ \\underset{\\beta}{\\min} \\sum_{i=1}^n (y_i - x_{i}^{\\prime}\\beta)^2 + \\frac{\\lambda}{n} \\lVert \\hat{\\Psi}\\beta \\rVert_1\n",
    "\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\hat{\\Psi} = diag(\\hat{\\psi_1},\\hat{\\psi_2},\\dots,\\hat{\\psi_p})$ are the data-driven penalty loadings chosen to be a function of the data depending on the setting. For more detail, you can check the [package documentation](https://arxiv.org/pdf/1608.00354)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470.23886001420425"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rlambda = model_rlasso[\"lambda0\"]\n",
    "rlambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. The predictive capability of each model (OLS, Lasso and RLasso) is reported via $MSE$ and $R^2$ out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18936386968849128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS \n",
    "\n",
    "y_predict_ols = GLM.predict(model_ls, [ones(size(X_flexible_test)[1]);;X_flexible_test])\n",
    "MSE_ols = mean((y_test-y_predict_ols).^2)\n",
    "R2_test_ols = 1-MSE_ols/var(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1848957453620037"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lasso CV\n",
    "\n",
    "y_predict_lasso = predict_model(model_lasso, X_flexible_test)\n",
    "MSE_lasso = mean((y_test-y_predict_lasso).^2)\n",
    "R2_test_lasso = 1-MSE_lasso/var(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18471427916881367"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rigurous Lasso\n",
    "intercept = ones(size(X_flexible_test,1))\n",
    "Z = [intercept;;X_flexible_test]\n",
    "y_predict_rlasso = Z*model_rlasso[\"coefficients\"]\n",
    "\n",
    "MSE_rlasso = mean((y_test-y_predict_rlasso).^2)\n",
    "R2_test_rlasso = 1-MSE_rlasso/var(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
